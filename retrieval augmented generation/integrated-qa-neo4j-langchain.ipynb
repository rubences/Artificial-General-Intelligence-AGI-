{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "658d16fe",
      "metadata": {
        "id": "658d16fe"
      },
      "source": [
        "# Enhanced Question Answering Integrating Unstructured and Graph Knowledge using Neo4j and LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b021828",
      "metadata": {
        "id": "7b021828"
      },
      "source": [
        "In this notebook, we walk through the implementation of a sophisticated question-answering system, leveraging the synergistic capabilities of Neo4j and LangChain. The step-by-step guide emphasises the process of integrating unstructured data and graph knowledge, ensuring a comprehensive understanding of utilizing Neo4j Vector Index and GraphCypherQAChain for enhanced, informed response generation with Mistral-7b."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3902a90e",
      "metadata": {
        "id": "3902a90e"
      },
      "source": [
        "![neo4j_mistral_architecture](../assets/img/neo4j_mistral_architecture.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0c44ae5",
      "metadata": {
        "id": "a0c44ae5"
      },
      "outputs": [],
      "source": [
        "%pip install langchain openai wikipedia tiktoken neo4j python-dotenv transformers\n",
        "%pip install -U sagemaker"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40b3c7a5",
      "metadata": {
        "id": "40b3c7a5"
      },
      "source": [
        "## Neo4j Vector Index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "354f4720",
      "metadata": {
        "id": "354f4720"
      },
      "source": [
        "We will start by importing the requisite libraries and modules, setting a foundation for interfacing with the dataset preparation, Neo4j Vector Index, and utilizing text generation capabilities of Mistral 7B. Utilizing dotenv,  it securely loads environment variables, safeguarding sensitive  credentials for the OpenAI API and Neo4j database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "095bb5ea",
      "metadata": {
        "id": "095bb5ea"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
        "from langchain.document_loaders import WikipediaLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "os.environ[\"NEO4J_URI\"] = os.getenv('NEO4J_URI')\n",
        "os.environ[\"NEO4J_USERNAME\"] = os.getenv('NEO4J_USERNAME')\n",
        "os.environ[\"NEO4J_PASSWORD\"] = os.getenv('NEO4J_PASSWORD')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e538758e",
      "metadata": {
        "id": "e538758e"
      },
      "source": [
        "Here, we decide to work with a  Wikipedia page of Leonhard Euler for our experiment. We use the bert-base-uncased model for tokenizing the text. The WikipediaLoader loads the raw content of the specified page, which is then chunked into smaller text pieces using RecursiveCharacterTextSplitter from LangChain. This splitter ensures that each chunk is maximized to  200 tokens with an overlap of 20 tokens, adhering to context window  limits for embedding models and making sure that the continuity of the context is not lost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e0ad588",
      "metadata": {
        "id": "6e0ad588",
        "outputId": "c3188268-2b96-4b7f-b88f-cc16063d7e0d",
        "colab": {
          "referenced_widgets": [
            "20ac33ea8f0a4deb9f0f6749bccf659e",
            "1845d6c247614ce8be5ac19285cf4818",
            "f5c98d1226dc49a79223aba9af841dd0",
            "9aac10733e354325bb7da37efbbc24ea"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20ac33ea8f0a4deb9f0f6749bccf659e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1845d6c247614ce8be5ac19285cf4818",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5c98d1226dc49a79223aba9af841dd0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9aac10733e354325bb7da37efbbc24ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def bert_len(text):\n",
        "    tokens = tokenizer.encode(text)\n",
        "    return len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9034b327",
      "metadata": {
        "id": "9034b327"
      },
      "outputs": [],
      "source": [
        "raw_documents = WikipediaLoader(query=\"Leonhard Euler\").load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "          chunk_size = 200,\n",
        "          chunk_overlap  = 20,\n",
        "          length_function = bert_len,\n",
        "          separators=['\\n\\n', '\\n', ' ', ''],\n",
        "      )\n",
        "\n",
        "documents = text_splitter.create_documents([raw_documents[0].page_content])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d1b255b",
      "metadata": {
        "id": "2d1b255b",
        "outputId": "68a83a76-e15e-4d67-d753-2e2775825776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17\n"
          ]
        }
      ],
      "source": [
        "print(len(documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718fbaba",
      "metadata": {
        "id": "718fbaba"
      },
      "source": [
        "The chunked documents are instantiated into the Neo4j vector index as nodes. It uses the core functionalities of Neo4j graph database and OpenAI embeddings to construct this vector index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0ce4118",
      "metadata": {
        "id": "b0ce4118"
      },
      "outputs": [],
      "source": [
        "# Instantiate Neo4j vector from documents\n",
        "neo4j_vector = Neo4jVector.from_documents(\n",
        "    documents,\n",
        "    OpenAIEmbeddings(),\n",
        "    url=os.environ[\"NEO4J_URI\"],\n",
        "    username=os.environ[\"NEO4J_USERNAME\"],\n",
        "    password=os.environ[\"NEO4J_PASSWORD\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2e50335",
      "metadata": {
        "id": "e2e50335"
      },
      "source": [
        "After ingesting the documents in the vector index, we perform vector similarity search for a sample user query and retrieve top2 most similar documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3461791e",
      "metadata": {
        "id": "3461791e",
        "outputId": "c0a18ae8-7690-4995-e1da-6c689b851416"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Early life ==\n",
            "Leonhard Euler was born on 15 April 1707, in Basel to Paul III Euler, a pastor of the Reformed Church, and Marguerite (née Brucker), whose ancestors include a number of well-known scholars in the classics. He was the oldest of four children, having two younger sisters, Anna Maria and Maria Magdalena, and a younger broth\n",
            "\n",
            "Leonhard Euler ( OY-lər, German: [ˈleːɔnhaʁt ˈɔʏlɐ] ; 15 April 1707 – 18 September 1783) was a Swiss mathematician, physicist, astronomer, geographer, logician, and engineer who founded the studies of graph theory and topology and\n"
          ]
        }
      ],
      "source": [
        "query = \"Who were the siblings of Leonhard Euler?\"\n",
        "vector_results = neo4j_vector.similarity_search(query, k=2)\n",
        "for i, res in enumerate(vector_results):\n",
        "    print(res.page_content)\n",
        "    if i != len(vector_results)-1:\n",
        "        print()\n",
        "vector_result = vector_results[0].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27956bd9",
      "metadata": {
        "id": "27956bd9"
      },
      "source": [
        "## Build Knowledge Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f86baea",
      "metadata": {
        "id": "7f86baea"
      },
      "source": [
        "Highly inspired by the NaLLM project, we use their open-source project to construct a knowledge graph from unstructured data. Below is a knowledge graph constructed using a single chunk of a document from Wikipedia article of Leonhard Euler."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9063f4f",
      "metadata": {
        "id": "e9063f4f"
      },
      "source": [
        "![p3_kg](../assets/img/p3_kg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8955a0b6",
      "metadata": {
        "id": "8955a0b6"
      },
      "source": [
        "## Neo4j DB QA chain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58998dfb",
      "metadata": {
        "id": "58998dfb"
      },
      "source": [
        "Next, we import the necessary libraries to setup the Neo4j DB QA Chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76cee590",
      "metadata": {
        "id": "76cee590"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import GraphCypherQAChain\n",
        "from langchain.graphs import Neo4jGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73c71e83",
      "metadata": {
        "id": "73c71e83"
      },
      "outputs": [],
      "source": [
        "graph = Neo4jGraph(\n",
        "    url=os.environ[\"NEO4J_URI\"], username=os.environ[\"NEO4J_USERNAME\"], password=os.environ[\"NEO4J_PASSWORD\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57cb5bc6",
      "metadata": {
        "id": "57cb5bc6"
      },
      "source": [
        "Once the graph is constructed, we need to connect to the Neo4jGraph instance and visualize the schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab94f953",
      "metadata": {
        "id": "ab94f953",
        "outputId": "c3e2ae58-7113-480c-cf95-681ea6284b6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "        Node properties are the following:\n",
            "        [{'labels': 'Person', 'properties': [{'property': 'name', 'type': 'STRING'}, {'property': 'nationality', 'type': 'STRING'}, {'property': 'death_date', 'type': 'STRING'}, {'property': 'birth_date', 'type': 'STRING'}]}, {'labels': 'Location', 'properties': [{'property': 'name', 'type': 'STRING'}]}, {'labels': 'Organization', 'properties': [{'property': 'name', 'type': 'STRING'}]}, {'labels': 'Publication', 'properties': [{'property': 'name', 'type': 'STRING'}]}]\n",
            "        Relationship properties are the following:\n",
            "        []\n",
            "        The relationships are the following:\n",
            "        ['(:Person)-[:worked_at]->(:Organization)', '(:Person)-[:influenced_by]->(:Person)', '(:Person)-[:born_in]->(:Location)', '(:Person)-[:lived_in]->(:Location)', '(:Person)-[:child_of]->(:Person)', '(:Person)-[:sibling_of]->(:Person)', '(:Person)-[:published]->(:Publication)']\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "print(graph.schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebecc347",
      "metadata": {
        "id": "ebecc347"
      },
      "source": [
        "The GraphCycherQAChain abstracts all the details and outputs a natural language response for a natural language question(NLQ). However, internally it uses LLMs to generate a Cypher query for an NLQ and retrieves graph result from the graph database and finally uses those result to generate the final natural language response, again using an LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "542da4d0",
      "metadata": {
        "id": "542da4d0"
      },
      "outputs": [],
      "source": [
        "chain = GraphCypherQAChain.from_llm(\n",
        "    ChatOpenAI(temperature=0), graph=graph, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a07cde4c",
      "metadata": {
        "id": "a07cde4c",
        "outputId": "9e8ebb5f-0ee1-4004-df8f-b9a0d3f8da43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mMATCH (p:Person {name: 'Leonhard Euler'})-[:sibling_of]->(sibling)\n",
            "RETURN sibling.name\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'sibling.name': 'Maria Magdalena'}, {'sibling.name': 'Anna Maria'}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "graph_result = chain.run(\"Who were the siblings of Leonhard Euler?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7c23d7f",
      "metadata": {
        "id": "f7c23d7f",
        "outputId": "6c9c7a20-fdb9-456b-9167-bde87f82c34b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The siblings of Leonhard Euler were Maria Magdalena and Anna Maria.'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph_result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55abfce7",
      "metadata": {
        "id": "55abfce7"
      },
      "source": [
        "## Mistral-7b-Instruct"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1dc0a2d",
      "metadata": {
        "id": "c1dc0a2d"
      },
      "source": [
        "We setup the Mistral-7B endpoint from Hugging Face within the AWS SageMaker environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a81b018",
      "metadata": {
        "id": "5a81b018"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import sagemaker\n",
        "import boto3\n",
        "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b44fa0e1",
      "metadata": {
        "id": "b44fa0e1"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    role = sagemaker.get_execution_role()\n",
        "except ValueError:\n",
        "    iam = boto3.client('iam')\n",
        "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
        "\n",
        "hub = {\n",
        "    'HF_MODEL_ID':'mistralai/Mistral-7B-Instruct-v0.1',\n",
        "    'SM_NUM_GPUS': json.dumps(1)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60445928",
      "metadata": {
        "id": "60445928"
      },
      "outputs": [],
      "source": [
        "huggingface_model = HuggingFaceModel(\n",
        "    image_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"),\n",
        "    env=hub,\n",
        "    role=role,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b3ed4a8",
      "metadata": {
        "id": "1b3ed4a8"
      },
      "source": [
        "The final response is crafted by constructing a prompt that includes an instruction, relevant data from the vector index, relevant information from the graph database, and the user's query. This prompt is then passed to the Mistral-7b model, which generates a meaningful and accurate response based on the provided information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cab75d0e",
      "metadata": {
        "id": "cab75d0e",
        "outputId": "9679b4ef-d3bd-4150-ae6b-1178c0b5d4f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------!"
          ]
        }
      ],
      "source": [
        "mistral7b_predictor = huggingface_model.deploy(\n",
        "    initial_instance_count=1,\n",
        "    instance_type=\"ml.g5.4xlarge\",\n",
        "    container_startup_health_check_timeout=300,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3295b25a",
      "metadata": {
        "id": "3295b25a",
        "outputId": "fd90ff3f-0961-474c-d672-28e2164c5438"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a helpful question-answering agent. Your task is to analyze \n",
            "and synthesize information from two sources: the top result from a similarity search \n",
            "(unstructured information) and relevant data from a graph database (structured information). \n",
            "Given the user's query: Who were the siblings of Leonhard Euler?, provide a meaningful and efficient answer based \n",
            "on the insights derived from the following data:\n",
            "\n",
            "Unstructured information: == Early life ==\n",
            "Leonhard Euler was born on 15 April 1707, in Basel to Paul III Euler, a pastor of the Reformed Church, and Marguerite (née Brucker), whose ancestors include a number of well-known scholars in the classics. He was the oldest of four children, having two younger sisters, Anna Maria and Maria Magdalena, and a younger broth. \n",
            "Structured information: The siblings of Leonhard Euler were Maria Magdalena and Anna Maria..\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = \"Who were the siblings of Leonhard Euler?\"\n",
        "final_prompt = f\"\"\"You are a helpful question-answering agent. Your task is to analyze\n",
        "and synthesize information from two sources: the top result from a similarity search\n",
        "(unstructured information) and relevant data from a graph database (structured information).\n",
        "Given the user's query: {query}, provide a meaningful and efficient answer based\n",
        "on the insights derived from the following data:\n",
        "\n",
        "Unstructured information: {vector_result}.\n",
        "Structured information: {graph_result}.\n",
        "\"\"\"\n",
        "\n",
        "print(final_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a577882f",
      "metadata": {
        "id": "a577882f",
        "outputId": "8d223eaf-a6ae-40cf-dbdd-ccfc6f4ccc7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The siblings of Leonhard Euler were Maria Magdalena and Anna Maria.\n"
          ]
        }
      ],
      "source": [
        "response = mistral7b_predictor.predict({\n",
        "    \"inputs\": final_prompt,\n",
        "})\n",
        "\n",
        "print(re.search(r\"Answer: (.+)\", response[0]['generated_text']).group(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41bf4e3b",
      "metadata": {
        "id": "41bf4e3b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "conda_pytorch_p310",
      "language": "python",
      "name": "conda_pytorch_p310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}