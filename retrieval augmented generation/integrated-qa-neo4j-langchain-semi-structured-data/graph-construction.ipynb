{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b475c8c",
   "metadata": {},
   "source": [
    "# Constructing knowledge graphs from text, tables, and images using OpenAI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7ec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain neo4j openai python-dotenv unstructured[all-docs] pydantic lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c5dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge poppler -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6a0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge tesseract -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747ead95",
   "metadata": {},
   "source": [
    "Import important and necessary libraries to run the entire project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5199ca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import requests\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from langchain.graphs.graph_document import (\n",
    "    Node as BaseNode,\n",
    "    Relationship as BaseRelationship\n",
    ")\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.pydantic_v1 import Field, BaseModel\n",
    "from unstructured.partition.auto import partition_pdf\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.graphs.graph_document import (\n",
    "    Node as BaseNode,\n",
    "    Relationship as BaseRelationship,\n",
    "    GraphDocument,\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from typing import List, Dict, Any, Optional\n",
    "from langchain.pydantic_v1 import Field, BaseModel\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca2f7b4",
   "metadata": {},
   "source": [
    "Load the env variables securely from .env file. Note we are using a different Neo4j instance for the graph creation and infformation retrieval. This allows us to make changes/manipulate the vector index and the graph database separately with no dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04920a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"NEO4J_URI\"] = os.getenv('NEO4J_URI2')\n",
    "os.environ[\"NEO4J_USERNAME\"] = os.getenv('NEO4J_USERNAME')\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv('NEO4J_PASSWORD2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4fc3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(\n",
    "    url=os.environ[\"NEO4J_URI\"],\n",
    "    username=os.environ[\"NEO4J_USERNAME\"],\n",
    "    password=os.environ[\"NEO4J_PASSWORD\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c6e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Property(BaseModel):\n",
    "  \"\"\"A single property consisting of key and value\"\"\"\n",
    "  key: str = Field(..., description=\"key\")\n",
    "  value: str = Field(..., description=\"value\")\n",
    "\n",
    "class Node(BaseNode):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of node properties\")\n",
    "\n",
    "class Relationship(BaseRelationship):\n",
    "    properties: Optional[List[Property]] = Field(\n",
    "        None, description=\"List of relationship properties\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06c2e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraph(BaseModel):\n",
    "    \"\"\"Generate a knowledge graph with entities and relationships.\"\"\"\n",
    "    nodes: List[Node] = Field(\n",
    "        ..., description=\"List of nodes in the knowledge graph\")\n",
    "    rels: List[Relationship] = Field(\n",
    "        ..., description=\"List of relationships in the knowledge graph\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b425e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_property_key(s: str) -> str:\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return s\n",
    "    first_word = words[0].lower()\n",
    "    capitalized_words = [word.capitalize() for word in words[1:]]\n",
    "    return \"\".join([first_word] + capitalized_words)\n",
    "\n",
    "def props_to_dict(props) -> dict:\n",
    "    \"\"\"Convert properties to a dictionary.\"\"\"\n",
    "    properties = {}\n",
    "    if not props:\n",
    "      return properties\n",
    "    for p in props:\n",
    "        properties[format_property_key(p.key)] = p.value\n",
    "    return properties\n",
    "\n",
    "def map_to_base_node(node: Node) -> BaseNode:\n",
    "    \"\"\"Map the KnowledgeGraph Node to the base Node.\"\"\"\n",
    "    properties = props_to_dict(node.properties) if node.properties else {}\n",
    "    # Add name property for better Cypher statement generation\n",
    "    properties[\"name\"] = node.id.title()\n",
    "    return BaseNode(\n",
    "        id=node.id.title(), type=node.type.capitalize(), properties=properties\n",
    "    )\n",
    "\n",
    "\n",
    "def map_to_base_relationship(rel: Relationship) -> BaseRelationship:\n",
    "    \"\"\"Map the KnowledgeGraph Relationship to the base Relationship.\"\"\"\n",
    "    source = map_to_base_node(rel.source)\n",
    "    target = map_to_base_node(rel.target)\n",
    "    properties = props_to_dict(rel.properties) if rel.properties else {}\n",
    "    return BaseRelationship(\n",
    "        source=source, target=target, type=rel.type, properties=properties\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2db5a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "def get_extraction_chain(\n",
    "    allowed_nodes: Optional[List[str]] = None,\n",
    "    allowed_rels: Optional[List[str]] = None\n",
    "    ):\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\n",
    "      \"system\",\n",
    "      f\"\"\"# Knowledge Graph Instructions for GPT-4\n",
    "## 1. Overview\n",
    "You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n",
    "- **Nodes** represent entities and concepts. They're akin to Wikipedia nodes.\n",
    "- The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
    "## 2. Labeling Nodes\n",
    "- **Consistency**: Ensure you use basic or elementary types for node labels.\n",
    "  - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n",
    "- **Node IDs**: Never utilize integers as node IDs. Node IDs should be names or human-readable identifiers found in the text.\n",
    "{'- **Allowed Node Labels:**' + \", \".join(allowed_nodes) if allowed_nodes else \"\"}\n",
    "{'- **Allowed Relationship Types**:' + \", \".join(allowed_rels) if allowed_rels else \"\"}\n",
    "## 3. Identifying and Processing Tables\n",
    "- **Table Detection**: Identify tables by the keyword \"Table\" in text document.\n",
    "- **Entity and Relationship Extraction**: From tables, extract entities and their relationships. Consider rows, columns, and headers for contextual understanding.\n",
    "## 4. Handling Image URI/Links\n",
    "Follow these rules if the text contains the keyword Image URI\n",
    "- **Mandatory Image URI in Each Node**: Each node in the document must include an 'ImageURI' attribute. This applies to all nodes, regardless of their type or content.\n",
    "- **Direct URI Integration**: Attach the Image URI directly as an attribute within each node. Do not create separate nodes for URIs.\n",
    "- **Consistent Key-Value Format**: Use a uniform key-value pair for the Image URI attribute across all nodes. The key should be 'ImageURI', and the value should be the actual URI link.\n",
    "- **No Exclusions**: Ensure every node, including text, image, sketch, etc., has an 'ImageURI' attribute. This is essential for visual representation and consistency across the knowledge graph.\n",
    "- **Specificity for Image Nodes**: For nodes starting with the 'Image' keyword, the 'ImageURI' attribute should directly reference the relevant image.\n",
    "## 5. Handling Numerical Data and Dates\n",
    "- Numerical data, like age or other related information, should be incorporated as attributes or properties of the respective nodes.\n",
    "- **No Separate Nodes for Dates/Numbers**: Do not create separate nodes for dates or numerical values. Always attach them as attributes or properties of nodes.\n",
    "- **Property Format**: Properties must be in a key-value format.\n",
    "- **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
    "- **Naming Convention**: Use camelCase for property keys, e.g., `birthDate`.\n",
    "## 6. Coreference Resolution\n",
    "- **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
    "If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"), \n",
    "always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.  \n",
    "Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial. \n",
    "## 7. Strict Compliance\n",
    "Adhere to the rules strictly. Non-compliance will result in termination.\"\"\"),\n",
    "        (\"human\", \"Use the given format to extract information from the following input: {input}\"),\n",
    "        (\"human\", \"Tip: Make sure to answer in the correct format\"),\n",
    "    ])\n",
    "    return create_structured_output_chain(KnowledgeGraph, llm, prompt, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "461e7ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_store_graph(\n",
    "    document: Document,\n",
    "    nodes:Optional[List[str]] = None,\n",
    "    rels:Optional[List[str]]=None) -> None:\n",
    "    # Extract graph data using OpenAI functions\n",
    "    extract_chain = get_extraction_chain(nodes, rels)\n",
    "    data = extract_chain.run(document.page_content)\n",
    "    # Construct a graph document\n",
    "    graph_document = GraphDocument(\n",
    "      nodes = [map_to_base_node(node) for node in data.nodes],\n",
    "      relationships = [map_to_base_relationship(rel) for rel in data.rels],\n",
    "      source = document\n",
    "    )\n",
    "    # Store information into a graph\n",
    "    graph.add_graph_documents([graph_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acee656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cea855c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05217e7ce56b45bc8d2de7e463ffc8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d52e3827e44fac82c0b8607b7d1d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/115M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef31597c43f49488df4c921a5a1de37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=path + \"gemini_1_report.pdf\",\n",
    "    # Using pdf format to find embedded image blocks\n",
    "    extract_images_in_pdf=True,\n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    # Hard max on chunks\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=path+'img',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bf68e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "\n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Tables\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "\n",
    "# Text\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "327429c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [i.text for i in text_elements]\n",
    "tables = [i.text for i in table_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c8f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3', region_name='us-east-1')\n",
    "bucket_name = 'neo4j-rag-img-s3'\n",
    "for img_file in sorted(os.listdir('./img')):\n",
    "    if img_file.endswith('.jpg'):\n",
    "        img_path = os.path.join('./img', img_file)\n",
    "        s3_client.upload_file(img_path, bucket_name, img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b136acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://neo4j-rag-img-s3.s3.amazonaws.com/figure-2-1.jpg\n",
      "https://neo4j-rag-img-s3.s3.amazonaws.com/figure-4-2.jpg\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3', region_name='us-east-1')\n",
    "bucket_name = 'neo4j-rag-img-s3'\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "image_uris = []\n",
    "if 'Contents' in response:\n",
    "    for item in response['Contents']:\n",
    "        file_name = item['Key']\n",
    "        if file_name.endswith('.jpg'):\n",
    "            # Construct the URI for each image file\n",
    "            image_uri = f\"https://{bucket_name}.s3.amazonaws.com/{file_name}\"\n",
    "            print(image_uri)\n",
    "            image_uris.append(image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfb3937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    ''' Getting the base64 string '''\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "def encode_image_from_uri(image_uri):\n",
    "    ''' Getting the base64 string from an image URI '''\n",
    "    response = requests.get(image_uri)\n",
    "    if response.status_code == 200:\n",
    "        return base64.b64encode(response.content).decode('utf-8')\n",
    "    else:\n",
    "        raise Exception(f\"Failed to process image.\")\n",
    "\n",
    "def image_summarize(img_base64,prompt):\n",
    "    ''' Image summary '''\n",
    "    chat = ChatOpenAI(model=\"gpt-4-vision-preview\",\n",
    "                      max_tokens=1024)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\":prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{img_base64}\"\n",
    "                        },\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "# Store base64 encoded images\n",
    "img_base64_list = []\n",
    "# Store image summaries\n",
    "image_summaries = []\n",
    "# Prompt\n",
    "prompt = \"Describe the image in detail. Be specific about graphs, such as bar plots.\"\n",
    "# Read images, encode to base64 strings\n",
    "for img_uri in image_uris:\n",
    "    base64_image = encode_image_from_uri(img_uri)\n",
    "    img_base64_list.append(base64_image)\n",
    "    image_summaries.append((image_summarize(base64_image,prompt), img_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58c8032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for text in texts:\n",
    "    val = Document(page_content=text)\n",
    "    documents.append(val)\n",
    "    \n",
    "for text in tables:\n",
    "    # add table for table identification\n",
    "    text = \"Table: Numerical values are important and must be treated as properties of associated nodes depending upon the context of rows and column: \" + text\n",
    "    val = Document(page_content=text)\n",
    "    documents.append(val)\n",
    "    \n",
    "for text, uri in image_summaries:\n",
    "    # add image and image uri for image summary identification\n",
    "    text = \"Image: Add ImageURI property \" + uri + \" to all the nodes extracted from this document: \" + text\n",
    "    val = Document(page_content=text)\n",
    "    documents.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f11b82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [06:32<00:00, 35.72s/it]\n"
     ]
    }
   ],
   "source": [
    "for i, d in tqdm(enumerate(documents), total=len(documents)):\n",
    "    clean_doc = d.page_content.replace(\"\\\\\", \"\")\n",
    "    d = Document(page_content=clean_doc)\n",
    "    extract_and_store_graph(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228147b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
