{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a0ec7e",
   "metadata": {},
   "source": [
    "# Enhanced Question Answering Integrating Unstructured and Graph Knowledge using Neo4j and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251d2b1",
   "metadata": {},
   "source": [
    "In this notebook, we walk through the implementation of a sophisticated question-answering system, leveraging the synergistic capabilities of Neo4j and LangChain. The step-by-step guide emphasises the process of integrating unstructured data and graph knowledge, ensuring a comprehensive understanding of utilizing Neo4j Vector Index and GraphCypherQAChain for enhanced, informed response generation with Mistral-7b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c426144",
   "metadata": {},
   "source": [
    "![semi-neo4j-langchain-pipeline.png](../../assets/img/semi-neo4j-langchain-pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82931536",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain unstructured[all-docs] pydantic lxml\n",
    "!pip install openai wikipedia tiktoken neo4j python-dotenv\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge poppler -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebd5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge tesseract -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d4847",
   "metadata": {},
   "source": [
    "Import important and necessary libraries to run the entire project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce453916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import getpass\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import base64\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "from operator import itemgetter\n",
    "from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.auto import partition_pdf\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Neo4jVector\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import uuid\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62ade8",
   "metadata": {},
   "source": [
    "Load the env variables securely from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce0b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"NEO4J_URI\"] = os.getenv('NEO4J_URI')\n",
    "os.environ[\"NEO4J_USERNAME\"] = os.getenv('NEO4J_USERNAME')\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv('NEO4J_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ea8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f119631",
   "metadata": {},
   "source": [
    "Use unstructured library to partition pdf into text, table, and image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "540f6948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=path + \"gemini_1_report.pdf\",\n",
    "    # Using pdf format to find embedded image blocks\n",
    "    extract_images_in_pdf=True,\n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    # Post processing to aggregate text once we have the title\n",
    "    chunking_strategy=\"by_title\",\n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    # Hard max on chunks\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=path+'img',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5aca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "\n",
    "# Categorize by type\n",
    "categorized_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "\n",
    "# Tables\n",
    "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
    "\n",
    "# Text\n",
    "text_elements = [e for e in categorized_elements if e.type == \"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4874d7",
   "metadata": {},
   "source": [
    "## Neo4j Vector Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9361f",
   "metadata": {},
   "source": [
    "Use the Parent-Child indexing and retrieval strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3448c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
    "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "916150d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to text\n",
    "texts = [i.text for i in text_elements]\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea6d8628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to tables\n",
    "tables = [i.text for i in table_elements]\n",
    "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb2e75e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3', region_name='us-east-1')\n",
    "bucket_name = 'neo4j-rag-img-s3'\n",
    "for img_file in sorted(os.listdir('./img')):\n",
    "    if img_file.endswith('.jpg'):\n",
    "        img_path = os.path.join('./img', img_file)\n",
    "        s3_client.upload_file(img_path, bucket_name, img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cb092a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    ''' Getting the base64 string '''\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    \n",
    "def encode_image_from_uri(image_uri):\n",
    "    ''' Getting the base64 string from an image URI '''\n",
    "    response = requests.get(image_uri)\n",
    "    if response.status_code == 200:\n",
    "        return base64.b64encode(response.content).decode('utf-8')\n",
    "    else:\n",
    "        raise Exception(f\"Failed to process image.\")\n",
    "\n",
    "def image_summarize(img_base64,prompt):\n",
    "    ''' Image summary '''\n",
    "    chat = ChatOpenAI(model=\"gpt-4-vision-preview\",\n",
    "                      max_tokens=1024)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\":prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{img_base64}\"\n",
    "                        },\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "# Store base64 encoded images\n",
    "img_base64_list = []\n",
    "# Store image summaries\n",
    "image_summaries = []\n",
    "# Prompt\n",
    "prompt = \"Describe the image in detail. Be specific about graphs, such as bar plots.\"\n",
    "# Read images, encode to base64 strings\n",
    "response = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "if 'Contents' in response:\n",
    "    for item in response['Contents']:\n",
    "        file_name = item['Key']\n",
    "        if file_name.endswith('.jpg'):\n",
    "            image_uri = f\"https://{bucket_name}.s3.amazonaws.com/{file_name}\"\n",
    "            base64_image = encode_image_from_uri(image_uri)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image,prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57370e2f",
   "metadata": {},
   "source": [
    "We will index the child chunks in a Neo4j vector index and store the parent documents in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89c4269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "index_name = \"vector\" \n",
    "vectorstore = Neo4jVector.from_existing_index(\n",
    "    OpenAIEmbeddings(),\n",
    "    url=os.environ[\"NEO4J_URI\"], \n",
    "    username=os.environ[\"NEO4J_USERNAME\"], \n",
    "    password=os.environ[\"NEO4J_PASSWORD\"],\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key\n",
    ")\n",
    "\n",
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "    for i, s in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "# Add image summaries\n",
    "img_ids = [str(uuid.uuid4()) for _ in img_base64_list]\n",
    "summary_img = [\n",
    "    Document(page_content=s, metadata={id_key: img_ids[i]})\n",
    "    for i, s in enumerate(image_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_img)\n",
    "retriever.docstore.mset(list(zip(img_ids, img_base64_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73b01085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gemini Gemini GPT-4 GPT-3.5 PaLM 2-L Claude 2 _Inflect- Grok1 — LLAMA-2 Ultra Pro ion-2 MMLU 90.04% 79.13% 87.29% 70% 78.4% 78.5% 79.6% 73.0% 68.0%*** Multiple-choice questions — CoT@32\" CoT@8* CoT@32 5-shot 5-shot 5-shot CoT —_5-shot 5-shot in 57 subjects (via API“) (professional & academic) 83.7% 71.8% 86.4% (Hendrycks et al., 2021a) —_5-shot 5-shot 5-shot (reported) GSM8K 94.4% 86.5% 92.0% 57.1% 80.0% 88.0% 81.4% 62.9% 56.8% Grade-school math Majl@32 Maj1@32 SFT & 5-shot 5-shot O-shot 8-shot 8-shot 5-shot (Cobbe et al., 2021) 5-shot CoT MATH 53.2% 32.6% 52.9% 34.1% 34.4% _ 34.8% 23.9% 13.5% Math problems across 4-shot 4-shot 4-shot 4-shot 4-shot 4-shot 4-shot 5 difficulty levels & (via API“*) (via API“) 7 subdisciplines (Hendrycks et al., 2021b) 50.3% (Zheng et al., 2023) BIG-Bench-Hard 83.6% 75.0% 83.1% 66.6% 77.7% _ _ _ 51.2% Subset of hard BIG-bench _3-shot 3-shot 3-shot 3-shot 3-shot 3-shot tasks written as CoT prob- (via API“*) (via API**) lems (Srivastava et al., 2022) HumanEval 74.4% 67.7% 67.0% 48.1% _— 70.0% 44.5% 63.2% 29.9% Python coding tasks O-shot (IT) O-shot (IT) 0-shot O-shot O-shot O-shot O-shot O-shot (Chen et al., 2021) (reported) Natural2Code 74.9% 69.6% 73.9% 62.3% Python code generation. _O-shot O-shot 0-shot O-shot (New held-out set with no (via API\") (via API\"*) leakage on web) DROP 82.4 74.1 80.9 64.1 82.0 Reading comprehension Variable Variable 3-shot 3-shot Variable & arithmetic. shots shots (reported) shots (metric: F1-score) (Dua et al., 2019) HellaSwag 87.8% 84.7% 95.3% 85.5% 86.8% _ 89.0% _ 80.0%*** (validation set) 10-shot 10-shot 10-shot 10-shot 10-shot 10-shot Common-sense multiple (reported) choice questions (Zellers et al., 2019) WMT23 74.4 71.7 73.8 _ 72.7 Machine translation (met- _1-shot (IT) L-shot L-shot L-shot ric: BLEURT) (via API\") (Tom et al. 2023)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fbae18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The table presents the performance of various AI models on different tasks. Gemini, GPT-4, GPT-3.5, PaLM 2-L, Claude 2, Inflect-Grok1, LLAMA-2, Ultra Pro, and ion-2 are evaluated on tasks like multiple-choice questions, grade-school math, math problems across difficulty levels and subdisciplines, subset of hard BIG-bench tasks, Python coding tasks, Python code generation, reading comprehension and arithmetic, common-sense multiple choice questions, and machine translation. The performance is measured in percentages, with Gemini generally outperforming the other models in most tasks.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c76ca7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.1. Text\\n\\n5.1.1. Academic Benchmarks\\n\\nWe compare Gemini Pro and Ultra to a suite of external LLMs and our previous best model PaLM 2 across a series of text-based academic benchmarks covering reasoning, reading comprehension, STEM, and coding. We report these results in Table 2. Broadly, we find that the performance of Gemini Pro outperforms inference-optimized models such as GPT-3.5 and performs comparably with several of the most capable models available, and Gemini Ultra outperforms all current models. In this section, we examine some of these findings.\\n\\nOn MMLU (Hendrycks et al., 2021a), Gemini Ultra can outperform all existing models, achieving an accuracy of 90.04%. MMLU is a holistic exam benchmark, which measures knowledge across a set of 57 subjects. Human expert performance is gauged at 89.8% by the benchmark authors, and Gemini Ultra is the first model to exceed this threshold, with the prior state-of-the-art result at 86.4%. Achieving high performance requires specialist knowledge across many domains (e.g. law, biology, history, etc.), alongside reading comprehension and reasoning. We find Gemini Ultra achieves highest accuracy when used in combination with a chain-of-thought prompting approach (Wei et al., 2022) that accounts for model uncertainty. The model produces a chain of thought with k samples, for example 8 or 32. If there is a consensus above a preset threshold (selected based on the validation split), it selects this answer, otherwise it reverts to a greedy sample based on maximum likelihood choice without chain of thought. We refer the reader to appendix for a detailed breakdown of how this approach compares with only chain-of-thought prompting or only greedy sampling.\\n\\nIn mathematics, a field commonly used to benchmark the analytical capabilities of models, Gemini Ultra shows strong performance on both elementary exams and competition-grade problem sets. For the grade-school math benchmark, GSM8K (Cobbe et al., 2021), we find Gemini Ultra reaches 94.4% accuracy with chain-of-thought prompting and self-consistency (Wang et al., 2022) compared to the previous best accuracy of 92% with the same prompting technique. Similar positive trends are observed in increased difficulty math problems drawn from middle- and high-school math competitions (MATH benchmark), with the Gemini Ultra model outperforming all competitor models, reaching 53.2% using 4-shot prompting. The model also outperforms the state of the art on even harder tasks derived from American Mathematical Competitions (150 questions from 2022 and 2023). Smaller models perform poorly on this challenging task scoring close to random, but Gemini Ultra can solve 32% of the questions, compared to the 30% solve rate for GPT-4.\\n\\nGemini Ultra also excels in coding, a popular use case of current LLMs. We evaluate the model on many conventional and internal benchmarks and also measure its performance as part of more complex reasoning systems such as AlphaCode 2 (see section 5.1.7 on complex reasoning systems). For example, on HumanEval, a standard code-completion benchmark (Chen et al., 2021) mapping function descriptions to Python implementations, instruction-tuned Gemini Ultra correctly implements 74.4% of problems. On a new held-out evaluation benchmark for python code generation tasks, Natural2Code, where we ensure no web leakage, Gemini Ultra achieves the highest score of 74.9%.\\n\\nGemini: A Family of Highly Capable Multimodal Models'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\n",
    "    \"Which model outperforms on the Mmlu benchmark and is very similar to Palm-2?\"\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19aa524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64decode\n",
    "def split_text_image(docs):\n",
    "    b64 = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        try:\n",
    "            b64decode(doc)\n",
    "            b64.append(doc)\n",
    "        except Exception as e:\n",
    "            text.append(doc)\n",
    "    return {\n",
    "        \"images\": b64,\n",
    "        \"texts\": text\n",
    "    }\n",
    "\n",
    "def is_base64(s):\n",
    "    try:\n",
    "        if not re.match('^[A-Za-z0-9+/]+[=]{0,2}$', s):\n",
    "            return False\n",
    "        if len(s) % 4 != 0:\n",
    "            return False\n",
    "        base64.b64decode(s, validate=True)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc032e",
   "metadata": {},
   "source": [
    "We will use the GPT-4V model for final generation as the retrieved information might contain image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4231a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt4v_prompt(dict):\n",
    "    format_texts = \"\\n\".join(dict[\"context\"][\"texts\"])\n",
    "    if len(dict['context']['images'])>0 and is_base64(dict['context']['images'][0]):\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": f\"\"\"Answer the question based only on the following context, which can include text, tables, and the below image:\n",
    "                Question: {dict[\"question\"]}\n",
    "\n",
    "                Text and tables:\n",
    "                {format_texts}\n",
    "                \"\"\"\n",
    "            },\n",
    "            {\"type\": \"image_url\", \n",
    "             \"image_url\": {\"url\": f\"data:image/jpeg;base64,{dict['context']['images'][0]}\"}},\n",
    "        ]\n",
    "    else:\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": f\"\"\"Answer the question based only on the following context, which can include text, tables, and the below image:\n",
    "                Question: {dict[\"question\"]}\n",
    "\n",
    "                Text and tables:\n",
    "                {format_texts}\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    return [\n",
    "            HumanMessage(\n",
    "                content=content\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "chain = (\n",
    "    {\"context\": retriever | RunnableLambda(split_text_image), \"question\": RunnablePassthrough()}\n",
    "    | RunnableLambda(gpt4v_prompt)\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60c9491a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gemini Ultra outperforms on the MMLU benchmark with an accuracy of 90.04%, which is very similar to Palm-2.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_result1 = chain.invoke(\n",
    "    \"Which model outperforms on the Mmlu benchmark and is very similar to Palm-2?\"\n",
    ")\n",
    "vector_result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca9ba39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Safety Filtering and Quality Filters are applied to all Gemini models, as mentioned in the \"Training Dataset\" section of the provided text.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_result2 = chain.invoke(\n",
    "    \"Which models apply Safety Filtering and Quality Filters\"\n",
    ")\n",
    "vector_result2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec07b67d",
   "metadata": {},
   "source": [
    "## Neo4j DB QA chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10c9cd9",
   "metadata": {},
   "source": [
    "Once the graph is constructed, we need to connect to the Neo4j instance and print the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35593887",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"NEO4J_URI\"] = os.getenv('NEO4J_URI2')\n",
    "os.environ[\"NEO4J_USERNAME\"] = os.getenv('NEO4J_USERNAME')\n",
    "os.environ[\"NEO4J_PASSWORD\"] = os.getenv('NEO4J_PASSWORD2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f052916",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(\n",
    "    url=os.environ[\"NEO4J_URI\"], \n",
    "    username=os.environ[\"NEO4J_USERNAME\"], \n",
    "    password=os.environ[\"NEO4J_PASSWORD\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6badc79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties are the following:\n",
      "Organization {name: STRING, id: STRING},Multimodal model {name: STRING, description: STRING, id: STRING},Team {name: STRING, affiliation: STRING, id: STRING},Model {name: STRING, version: STRING, id: STRING, sizes: STRING, performance: STRING, modelDescription: STRING, modelSize: STRING},Modelsize {id: STRING, name: STRING, description: STRING},Benchmark {description: STRING, id: STRING, name: STRING, accuracy: STRING},Person {id: STRING, name: STRING},Concept {id: STRING, name: STRING},Model family {id: STRING, name: STRING},Accelerator {id: STRING, name: STRING},Reference {name: STRING, id: STRING, year: STRING, authors: STRING},Challenge {name: STRING, description: STRING, id: STRING},Hardware {name: STRING, description: STRING, id: STRING},Network {description: STRING, id: STRING, references: STRING, name: STRING},Training paradigm {name: STRING, description: STRING, id: STRING},Programming model {description: STRING, id: STRING, name: STRING},Compiler {name: STRING, description: STRING, id: STRING},Technique {name: STRING, id: STRING, description: STRING},Problem {description: STRING, id: STRING, references: STRING, name: STRING},Dataset {id: STRING, name: STRING},Tokenizer {name: STRING, id: STRING},Filter {name: STRING, id: STRING},Mixture {id: STRING, name: STRING},Weight {id: STRING, name: STRING},Metric {id: STRING, name: STRING},Table {id: STRING, name: STRING},Parameter {id: STRING, name: STRING},Technology {id: STRING, name: STRING},Date {id: STRING, name: STRING},Researcher {name: STRING, id: STRING},Node {name: STRING, id: STRING, numericalvalue: STRING, imageuri: STRING},Text {name: STRING, content: STRING, id: STRING},Image {name: STRING, imageuri: STRING, id: STRING},Chunk {doc_id: STRING, id: STRING, text: STRING, embedding: LIST}\n",
      "Relationship properties are the following:\n",
      "PRODUCES_CHAIN_OF_THOUGHT_WITH_K_SAMPLES {k: STRING},PRODUCES_CHAIN_OF_THOUGHT_WITH_CONSENSUS_THRESHOLD {threshold: STRING},RESULTS_COLLECTED_VIA {date: STRING},ADDITIONAL_FINETUNING_STEPS_IMPROVE_VALIDATION_ACCURACY {geminiPro: STRING, geminiUltra: STRING, gpt-4: STRING}\n",
      "The relationships are the following:\n",
      "(:Multimodal model)-[:DEVELOPED_BY]->(:Organization),(:Team)-[:AFFILIATED_WITH]->(:Organization),(:Model)-[:PRODUCES_CHAIN_OF_THOUGHT_WITH_CONSENSUS_THRESHOLD]->(:Parameter),(:Model)-[:HASSIZE]->(:Modelsize),(:Model)-[:HASVERSION]->(:Model),(:Model)-[:PRODUCES_CHAIN_OF_THOUGHT_WITH_K_SAMPLES]->(:Parameter),(:Model)-[:USED]->(:Accelerator),(:Model)-[:CITED]->(:Reference),(:Model)-[:OUTPERFORMS]->(:Benchmark),(:Model)-[:COMPAREDTO]->(:Model),(:Model)-[:ACHIEVESPERFORMANCE]->(:Benchmark),(:Model)-[:BELONGS_TO]->(:Model family),(:Model)-[:APPLY_FILTERS]->(:Filter),(:Model)-[:TRAINED_ON]->(:Dataset),(:Model)-[:USES_TOKENIZER]->(:Tokenizer),(:Model)-[:TRAINED_FOR]->(:Model),(:Model)-[:FILTER_FROM]->(:Dataset),(:Model)-[:DETERMINE_MIXTURE]->(:Mixture),(:Model)-[:DEFINE_METRIC]->(:Metric),(:Model)-[:COMPARED_TO]->(:Model),(:Person)-[:IDENTIFIEDMISTAKE]->(:Concept),(:Person)-[:CREATED]->(:Concept),(:Concept)-[:HAS]->(:Concept),(:Concept)-[:EXPLANATION]->(:Concept),(:Concept)-[:CALCULATION]->(:Concept),(:Concept)-[:USES]->(:Concept),(:Concept)-[:CALCULATES]->(:Concept),(:Concept)-[:BUILD_ON_TOP_OF]->(:Concept),(:Concept)-[:INGESTS_AUDIO_SIGNALS_FROM]->(:Concept),(:Model family)-[:INSPIRED_BY]->(:Concept),(:Challenge)-[:RESULTSIN]->(:Model),(:Hardware)-[:DEPLOYEDIN]->(:Model),(:Network)-[:USEDFOR]->(:Model),(:Training paradigm)-[:USEDIN]->(:Model),(:Programming model)-[:USEDFOR]->(:Model),(:Compiler)-[:USEDIN]->(:Model),(:Technique)-[:USEDIN]->(:Model),(:Problem)-[:ADDRESSES]->(:Model),(:Problem)-[:CITED]->(:Reference),(:Mixture)-[:DETERMINE_WEIGHT]->(:Weight),(:Table)-[:RESULTS_COMPARISON]->(:Researcher),(:Table)-[:PERFORMANCE_ON_BENCHMARKS]->(:Model),(:Table)-[:RESULTS_COLLECTED_VIA]->(:Technology),(:Table)-[:NOT_REPORTED_RESULTS_ON]->(:Researcher),(:Table)-[:NOT_REPORTED_RESULTS_ON]->(:Benchmark),(:Table)-[:ADDITIONAL_FINETUNING_STEPS_IMPROVE_VALIDATION_ACCURACY]->(:Benchmark),(:Table)-[:REPORT_DECONTAMINATED_RESULTS_ONLY_IN_10_SHOT_EVALUATION_SETTING]->(:Benchmark),(:Table)-[:EVALUATE_ON_NEW_HELD_OUT_EVALUATION_DATASETS]->(:Benchmark),(:Table)-[:BENCHMARK_RESULTS_SUSCEPTIBLE_TO_PRETRAINING_DATASET_COMPOSITION]->(:Researcher),(:Node)-[:PROCESS]->(:Node),(:Node)-[:OUTPUT]->(:Node),(:Image)-[:HASERROR]->(:Text),(:Image)-[:HASINSTRUCTION]->(:Text),(:Image)-[:HASPROBLEM]->(:Text),(:Image)-[:HASDIAGRAM]->(:Text),(:Image)-[:HASPROMPT]->(:Text),(:Image)-[:HASSOLUTION]->(:Text)\n"
     ]
    }
   ],
   "source": [
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1bd1e4",
   "metadata": {},
   "source": [
    "We are modifying the CYPHER_GENERATION_TEMPLATE and the CYPHER_QA_TEMPLATE template. This is because we have image uris associated with certain nodes in the graph and we aim to retrieve them and pass them to gpt4v for potentially better output response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab18983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CYPHER_GENERATION_TEMPLATE = \"\"\"Task: Generate Cypher statement to query a graph database.\n",
    "Instructions:\n",
    "Use only the provided relationship types and properties in the schema.\n",
    "Do not use any other relationship types or properties that are not provided.\n",
    "Schema:\n",
    "{schema}\n",
    "Do not include any explanations or apologies in your responses.\n",
    "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
    "Do not include any text except the generated Cypher statement.\n",
    "Only if the node associated with the result cypher query contains 'imageuri' property return it as well.\n",
    "\n",
    "The question is:\n",
    "{question}\"\"\"\n",
    "CYPHER_GENERATION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"schema\", \"question\"], template=CYPHER_GENERATION_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2317d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "CYPHER_QA_TEMPLATE = \"\"\"You are an assistant that helps to form nice and human understandable answers.\n",
    "The information part contains the provided information that you must use to construct an answer.\n",
    "If the context contains link/imageuri, include it in the final answer output.\n",
    "The provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\n",
    "Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n",
    "If the provided information is empty, say that you don't know the answer.\n",
    "Information:\n",
    "{context}\n",
    "\n",
    "Examples:\n",
    "Question: What is the most fastest car?\n",
    "Helpful Answer: The fastest car on the list is Bugatti Chiron. Image URI: https://manofmany.com/wp-content/uploads/2019/09/Bugatti-Chiron.jpg\n",
    "\n",
    "Question: What is the most fastest car on Mars?\n",
    "Helpful Answer: The fastest car on Mars is unknown. Image URI: None\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "CYPHER_QA_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], template=CYPHER_QA_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "746470cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_chain = GraphCypherQAChain.from_llm(\n",
    "    ChatOpenAI(temperature=0), \n",
    "    graph=graph, \n",
    "    cypher_prompt=CYPHER_GENERATION_PROMPT, \n",
    "    qa_prompt=CYPHER_QA_PROMPT,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eccb533f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (m:Model)-[:OUTPERFORMS]->(b:Benchmark {name: \"Mmlu\"})\n",
      "MATCH (m)-[:COMPAREDTO]->(m2:Model {name: \"Palm-2\"})\n",
      "RETURN m.name, m.imageuri\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'m.name': 'Gemini Ultra', 'm.imageuri': None}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "graph_result1 = graph_chain.run(\"Which model outperforms on the Mmlu benchmark and is very similar to Palm-2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74d6ae1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The model that outperforms on the Mmlu benchmark and is very similar to Palm-2 is Gemini Ultra. Image URI: None'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab691e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (m:Model)-[:APPLY_FILTERS]->(f:Filter)\n",
      "WHERE f.name = 'Safety Filtering' OR f.name = 'Quality Filters'\n",
      "RETURN m.name, m.id, m.imageuri\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'m.name': 'Gemini Models', 'm.id': 'Gemini Models', 'm.imageuri': None}, {'m.name': 'Gemini Models', 'm.id': 'Gemini Models', 'm.imageuri': None}]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "graph_result2 = graph_chain.run(\"Which models apply Safety Filtering and Quality Filters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c662f471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gemini Models apply Safety Filtering and Quality Filters. Image URI: None'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_result2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c14b5",
   "metadata": {},
   "source": [
    "We split the text and imageuri same as before because thats the format in which the gpt-4v model api requires us to pass text and image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4975bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_image(dict):\n",
    "    res = dict['graph_result']\n",
    "    http_index = res.find('http')\n",
    "\n",
    "    if http_index != -1:\n",
    "        image_url = res[http_index:].strip()\n",
    "        text = res[:http_index].strip()\n",
    "\n",
    "        return {\n",
    "            \"question\": dict['question'],\n",
    "            \"image\": image_url,\n",
    "            \"text\": text.replace(\" Image URI:\", \"\")\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"question\": dict['question'],\n",
    "            \"image\": None,\n",
    "            \"text\": res.replace(\" Image URI:\", \"\")\n",
    "        }\n",
    "\n",
    "def is_base64(s):\n",
    "    try:\n",
    "        if not re.match('^[A-Za-z0-9+/]+[=]{0,2}$', s):\n",
    "            return False\n",
    "        if len(s) % 4 != 0:\n",
    "            return False\n",
    "        base64.b64decode(s, validate=True)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "    \n",
    "def encode_image_from_uri(image_uri):\n",
    "    ''' Getting the base64 string from an image URI '''\n",
    "    response = requests.get(image_uri)\n",
    "    if response.status_code == 200:\n",
    "        return base64.b64encode(response.content).decode('utf-8')\n",
    "    else:\n",
    "        raise Exception(f\"Failed to process image.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d052f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt4v_prompt(dict):\n",
    "    if dict['image']!=None and is_base64(b64_img):\n",
    "        b64_img = encode_image_from_uri(dict['image'])\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": f\"\"\"Answer the question based only on the following context, which can include text, tables, and the below image:\n",
    "                Question: {dict[\"question\"]}\n",
    "\n",
    "                Text and tables:\n",
    "                {dict['text']}\n",
    "                \"\"\"\n",
    "            },\n",
    "            {\"type\": \"image_url\", \n",
    "             \"image_url\": {\"url\": f\"data:image/jpeg;base64,{b64_img}\"}},\n",
    "        ]\n",
    "    else:\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": f\"\"\"Answer the question based only on the following context, which can include text, tables, and the below image:\n",
    "                Question: {dict[\"question\"]}\n",
    "\n",
    "                Text and tables:\n",
    "                {dict['text']}\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "    return [\n",
    "            HumanMessage(\n",
    "                content=content\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"graph_result\": itemgetter(\"graph_result\")\n",
    "    }\n",
    "    | RunnableLambda(split_text_image)\n",
    "    | RunnableLambda(gpt4v_prompt)\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65f7ba4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The model that outperforms on the Mmlu benchmark and is very similar to Palm-2 is Gemini Ultra. '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = split_text_image({\"graph_result\": graph_result1, \"question\": \"Which model outperforms on the Mmlu benchmark and is very similar to Palm-2?\"})\n",
    "if res[\"image\"]!=None:\n",
    "    graph_result1 = chain.invoke(\n",
    "        {\"question\": \"Which model outperforms on the Mmlu benchmark and is very similar to Palm-2?\", \"graph_result\": graph_result1}\n",
    "    )\n",
    "else:\n",
    "    graph_result1 = graph_result1.replace(\"Image URI: None\", \"\")\n",
    "graph_result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d226535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gemini Models apply Safety Filtering and Quality Filters. '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = split_text_image({\"graph_result\": graph_result2, \"question\": \"Which models apply Safety Filtering and Quality Filters\"})\n",
    "if res[\"image\"]!=None:\n",
    "    graph_result2 = chain.invoke(\n",
    "        {\"question\": \"Which models apply Safety Filtering and Quality Filters\", \"graph_result\": graph_result2}\n",
    "    )\n",
    "else:\n",
    "    graph_result2 = graph_result2.replace(\"Image URI: None\", \"\")\n",
    "graph_result2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42430a85",
   "metadata": {},
   "source": [
    "## Mistral-7b-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57e1d36",
   "metadata": {},
   "source": [
    "We setup the Mistral-7B endpoint from Hugging Face within the AWS SageMaker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d63fc12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "97185157",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "    'SM_NUM_GPUS': json.dumps(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c8fd850",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_model = HuggingFaceModel(\n",
    "    image_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\"),\n",
    "    env=hub,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb478b4",
   "metadata": {},
   "source": [
    "The final response is crafted by constructing a prompt that includes an instruction, relevant data from the vector index, relevant information from the graph database, and the user's query. This prompt is then passed to the Mistral-7b model, which generates a meaningful and accurate response based on the provided information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d1a4c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "mistral7b_predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.4xlarge\",\n",
    "    container_startup_health_check_timeout=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6fcc82b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini Ultra outperforms on the MMLU benchmark and is very similar\n"
     ]
    }
   ],
   "source": [
    "query = \"Which model outperforms on the Mmlu benchmark and is very similar to Palm-2?\"\n",
    "final_prompt = f\"\"\"<s>[INST]You are a helpful question-answering agent. Use the below \n",
    "context to answer the question:\n",
    "\n",
    "Context1: {vector_result1}\n",
    "Context2: {graph_result1}\n",
    "\n",
    "Question: {query}\n",
    "Answer:[/INST]\n",
    "\"\"\"\n",
    "\n",
    "response = mistral7b_predictor.predict({\n",
    "    \"inputs\": final_prompt,\n",
    "})\n",
    "\n",
    "print(re.search(r\"Answer:\\[\\s*/INST\\s*\\]\\n\\n(.+)\", response[0]['generated_text']).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "191a2ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini models apply Safety Filtering and Quality Filters.\n"
     ]
    }
   ],
   "source": [
    "query = \"Which models apply Safety Filtering and Quality Filters\"\n",
    "final_prompt = f\"\"\"<s>[INST]You are a helpful question-answering agent. Use the below \n",
    "context to answer the question:\n",
    "\n",
    "Context1: {vector_result2}\n",
    "Context2: {graph_result2}\n",
    "\n",
    "Question: {query}\n",
    "Answer:[/INST]\n",
    "\"\"\"\n",
    "\n",
    "response = mistral7b_predictor.predict({\n",
    "    \"inputs\": final_prompt,\n",
    "})\n",
    "\n",
    "print(re.search(r\"Answer:\\[\\s*/INST\\s*\\]\\n\\n(.+)\", response[0]['generated_text']).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02eef49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
